{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "#from keras.layers import Input, Dense, Conv2D, MaxPooling2D, concatenate, Flatten, AveragePooling2D\n",
    "from keras.layers import*\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.optimizers import*\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epoch = 20\n",
    "def poly_decay(epoch):\n",
    "    base_lr = 0.01\n",
    "    lr = base_lr*((1-(epoch/max_epoch))**0.5)\n",
    "    return lr\n",
    "lrate = LearningRateScheduler(poly_decay)\n",
    "callbacks_list = [lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = np.load(\"Xtrain.npy\")\n",
    "Xtest = np.load(\"Xtest.npy\")\n",
    "Xval = np.load(\"Xval.npy\")\n",
    "Ytrain = np.load(\"Ytrain.npy\")\n",
    "Ytest = np.load(\"Ytest.npy\")\n",
    "Yval = np.load(\"Yval.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  if sys.path[0] == '':\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  del sys.path[0]\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  \n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "  app.launch_new_instance()\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(208, 3, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, 5, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 1, padding=\"same\", activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4096, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1024, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/home/amitoj/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(7, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_input (InputLayer)          (None, 40, 40, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 20, 20, 64)   3200        img_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 5, 5, 192)    110784      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 3, 3, 192)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 96)     165984      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 3, 3, 16)     76816       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 192)    0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 3, 3, 64)     12352       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 3, 3, 128)    110720      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 3, 3, 32)     12832       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 3, 3, 32)     6176        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 3, 256)    0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 3, 3, 128)    295040      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 3, 3, 32)     204832      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 256)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 3, 3, 128)    32896       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 3, 3, 192)    221376      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 3, 3, 96)     76896       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 3, 3, 64)     16448       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 3, 480)    0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 2, 2, 480)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 2, 2, 96)     414816      max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 2, 2, 16)     192016      max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 2, 2, 480)    0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 2, 2, 192)    92352       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2, 2, 208)    179920      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 2, 2, 48)     19248       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 2, 2, 64)     30784       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2, 2, 512)    0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4096)         2101248     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         4195328     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7)            7175        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,579,239\n",
      "Trainable params: 8,579,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_input = Input(shape=(40,40,1), name='img_input')\n",
    "x = Conv2D(64,7, strides=(2,2), activation = 'relu',padding='same')(img_input)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2,2),padding='same')(x)   \n",
    "x = Conv2D(192,3,strides=(2,2),padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2,2),padding='same')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "\n",
    "#first inception layer 3a\n",
    "x1 = Conv2D(64,1, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #1*1\n",
    "x2 = Conv2D(96,3, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #3*3 reduce\n",
    "x2 = Conv2D(128,3, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x2) #3*3 \n",
    "x3 = Conv2D(16,5, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #5*5 reduce\n",
    "x3 = Conv2D(32,5, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x3) #5*5 \n",
    "x4 = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(x) #pool proj\n",
    "x4 = Conv2D(32,1, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x4) #1*1\n",
    "x = concatenate([x1,x2,x3,x4], axis = 3)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#second inception layer 3b\n",
    "x1 = Conv2D(128,1, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #1*1\n",
    "x2 = Conv2D(128,3,padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #3*3 reduce\n",
    "x2 = Conv2D(192,3, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x2) #3*3 \n",
    "x3 = Conv2D(32,5, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #5*5 reduce\n",
    "x3 = Conv2D(96,5, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x3) #5*5 \n",
    "x4 = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(x) #pool proj\n",
    "x4 = Conv2D(64,1, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x4)\n",
    "x = concatenate([x1,x2,x3,x4], axis = 3)\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=(2,2),padding='same')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "#third inception layer 4a\n",
    "x1 = Conv2D(192,1, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #1*1\n",
    "x2 = Conv2D(96,3,padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #3*3 reduce\n",
    "x2 = Conv2D(208,3, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x2) #3*3 \n",
    "x3 = Conv2D(16,5, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x) #5*5 reduce\n",
    "x3 = Conv2D(48,5, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x3) #5*5 \n",
    "x4 = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(x) #pool proj\n",
    "x4 = Conv2D(64,1, padding='same', activation = 'relu', W_regularizer=l2(0.0002))(x4) #1*1\n",
    "x = concatenate([x1,x2,x3,x4], axis = 3)\n",
    "#x = BatchNormalization()(x)\n",
    "#x = MaxPooling2D(pool_size=(3, 3), strides=(2,2),padding='same')(x)\n",
    "x = AveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "#x = Dropout(0.4)(x)\n",
    "x = Dense(4096, activation='relu', W_regularizer=l2(0.0002))(x)\n",
    "x = Dense(1024, activation='relu', W_regularizer=l2(0.0002))(x)\n",
    "x = Dense(7, activation='softmax', W_regularizer=l2(0.0002))(x)\n",
    "final_model = Model(inputs = [img_input], outputs = [x])\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, nesterov=False)\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 287100 samples, validate on 35890 samples\n",
      "Epoch 1/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 2.6952 - acc: 0.3179\n",
      "Epoch 00001: val_acc improved from -inf to 0.36361, saving model to weights/batchnorm-drop-01-0.3636.hdf5\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.36361, saving model to weights/batchnorm-drop-01-0.3636.hdf5\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.36361, saving model to weights/paper-spec-01-0.3636.hdf5\n",
      "287100/287100 [==============================] - 1747s 6ms/step - loss: 2.6952 - acc: 0.3179 - val_loss: 2.4846 - val_acc: 0.3636\n",
      "Epoch 2/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 2.3541 - acc: 0.4145\n",
      "Epoch 00002: val_acc improved from 0.36361 to 0.43575, saving model to weights/batchnorm-drop-02-0.4357.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.36361 to 0.43575, saving model to weights/batchnorm-drop-02-0.4357.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.36361 to 0.43575, saving model to weights/paper-spec-02-0.4357.hdf5\n",
      "287100/287100 [==============================] - 1746s 6ms/step - loss: 2.3542 - acc: 0.4145 - val_loss: 2.2950 - val_acc: 0.4357\n",
      "Epoch 3/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 2.2183 - acc: 0.4616\n",
      "Epoch 00003: val_acc improved from 0.43575 to 0.45848, saving model to weights/batchnorm-drop-03-0.4585.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.43575 to 0.45848, saving model to weights/batchnorm-drop-03-0.4585.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.43575 to 0.45848, saving model to weights/paper-spec-03-0.4585.hdf5\n",
      "287100/287100 [==============================] - 1749s 6ms/step - loss: 2.2183 - acc: 0.4615 - val_loss: 2.2044 - val_acc: 0.4585\n",
      "Epoch 4/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 2.1103 - acc: 0.4944\n",
      "Epoch 00004: val_acc improved from 0.45848 to 0.48036, saving model to weights/batchnorm-drop-04-0.4804.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.45848 to 0.48036, saving model to weights/batchnorm-drop-04-0.4804.hdf5\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.45848 to 0.48036, saving model to weights/paper-spec-04-0.4804.hdf5\n",
      "287100/287100 [==============================] - 1760s 6ms/step - loss: 2.1103 - acc: 0.4944 - val_loss: 2.1331 - val_acc: 0.4804\n",
      "Epoch 5/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 2.0200 - acc: 0.5201\n",
      "Epoch 00005: val_acc improved from 0.48036 to 0.50627, saving model to weights/batchnorm-drop-05-0.5063.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.48036 to 0.50627, saving model to weights/batchnorm-drop-05-0.5063.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.48036 to 0.50627, saving model to weights/paper-spec-05-0.5063.hdf5\n",
      "287100/287100 [==============================] - 1786s 6ms/step - loss: 2.0200 - acc: 0.5201 - val_loss: 2.0536 - val_acc: 0.5063\n",
      "Epoch 6/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.9415 - acc: 0.5416\n",
      "Epoch 00006: val_acc improved from 0.50627 to 0.52009, saving model to weights/batchnorm-drop-06-0.5201.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.50627 to 0.52009, saving model to weights/batchnorm-drop-06-0.5201.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.50627 to 0.52009, saving model to weights/paper-spec-06-0.5201.hdf5\n",
      "287100/287100 [==============================] - 1769s 6ms/step - loss: 1.9415 - acc: 0.5416 - val_loss: 2.0152 - val_acc: 0.5201\n",
      "Epoch 7/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.8680 - acc: 0.5612\n",
      "Epoch 00007: val_acc did not improve\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "287100/287100 [==============================] - 1791s 6ms/step - loss: 1.8680 - acc: 0.5612 - val_loss: 1.9902 - val_acc: 0.5172\n",
      "Epoch 8/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.8004 - acc: 0.5798\n",
      "Epoch 00008: val_acc did not improve\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "287100/287100 [==============================] - 1750s 6ms/step - loss: 1.8005 - acc: 0.5798 - val_loss: 2.0053 - val_acc: 0.5178\n",
      "Epoch 9/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.7366 - acc: 0.5969\n",
      "Epoch 00009: val_acc improved from 0.52009 to 0.52463, saving model to weights/batchnorm-drop-09-0.5246.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.52009 to 0.52463, saving model to weights/batchnorm-drop-09-0.5246.hdf5\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.52009 to 0.52463, saving model to weights/paper-spec-09-0.5246.hdf5\n",
      "287100/287100 [==============================] - 1766s 6ms/step - loss: 1.7366 - acc: 0.5969 - val_loss: 1.9559 - val_acc: 0.5246\n",
      "Epoch 10/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.6725 - acc: 0.6158\n",
      "Epoch 00010: val_acc improved from 0.52463 to 0.53012, saving model to weights/batchnorm-drop-10-0.5301.hdf5\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.52463 to 0.53012, saving model to weights/batchnorm-drop-10-0.5301.hdf5\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.52463 to 0.53012, saving model to weights/paper-spec-10-0.5301.hdf5\n",
      "287100/287100 [==============================] - 1784s 6ms/step - loss: 1.6725 - acc: 0.6158 - val_loss: 1.9856 - val_acc: 0.5301\n",
      "Epoch 11/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.6096 - acc: 0.6345\n",
      "Epoch 00011: val_acc improved from 0.53012 to 0.53277, saving model to weights/batchnorm-drop-11-0.5328.hdf5\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.53012 to 0.53277, saving model to weights/batchnorm-drop-11-0.5328.hdf5\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.53012 to 0.53277, saving model to weights/paper-spec-11-0.5328.hdf5\n",
      "287100/287100 [==============================] - 1832s 6ms/step - loss: 1.6096 - acc: 0.6345 - val_loss: 1.9562 - val_acc: 0.5328\n",
      "Epoch 12/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.5509 - acc: 0.6526\n",
      "Epoch 00012: val_acc did not improve\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "287100/287100 [==============================] - 1721s 6ms/step - loss: 1.5509 - acc: 0.6526 - val_loss: 1.9639 - val_acc: 0.5290\n",
      "Epoch 13/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.4912 - acc: 0.6717\n",
      "Epoch 00013: val_acc improved from 0.53277 to 0.53798, saving model to weights/batchnorm-drop-13-0.5380.hdf5\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.53277 to 0.53798, saving model to weights/batchnorm-drop-13-0.5380.hdf5\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.53277 to 0.53798, saving model to weights/paper-spec-13-0.5380.hdf5\n",
      "287100/287100 [==============================] - 1714s 6ms/step - loss: 1.4912 - acc: 0.6717 - val_loss: 1.9494 - val_acc: 0.5380\n",
      "Epoch 14/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.4387 - acc: 0.6894\n",
      "Epoch 00014: val_acc improved from 0.53798 to 0.54377, saving model to weights/batchnorm-drop-14-0.5438.hdf5\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.53798 to 0.54377, saving model to weights/batchnorm-drop-14-0.5438.hdf5\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.53798 to 0.54377, saving model to weights/paper-spec-14-0.5438.hdf5\n",
      "287100/287100 [==============================] - 1752s 6ms/step - loss: 1.4387 - acc: 0.6894 - val_loss: 1.9988 - val_acc: 0.5438\n",
      "Epoch 15/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.3823 - acc: 0.7086\n",
      "Epoch 00015: val_acc did not improve\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "287100/287100 [==============================] - 1761s 6ms/step - loss: 1.3823 - acc: 0.7086 - val_loss: 2.1116 - val_acc: 0.5352\n",
      "Epoch 16/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.3335 - acc: 0.7275\n",
      "Epoch 00016: val_acc did not improve\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "287100/287100 [==============================] - 1756s 6ms/step - loss: 1.3335 - acc: 0.7275 - val_loss: 2.1691 - val_acc: 0.5349\n",
      "Epoch 17/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.2846 - acc: 0.7448\n",
      "Epoch 00017: val_acc did not improve\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "287100/287100 [==============================] - 1790s 6ms/step - loss: 1.2846 - acc: 0.7448 - val_loss: 2.1746 - val_acc: 0.5363\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.2396 - acc: 0.7622\n",
      "Epoch 00018: val_acc did not improve\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "287100/287100 [==============================] - 1781s 6ms/step - loss: 1.2396 - acc: 0.7622 - val_loss: 2.1788 - val_acc: 0.5321\n",
      "Epoch 19/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.1968 - acc: 0.7790\n",
      "Epoch 00019: val_acc did not improve\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "287100/287100 [==============================] - 1740s 6ms/step - loss: 1.1968 - acc: 0.7790 - val_loss: 2.2135 - val_acc: 0.5388\n",
      "Epoch 20/20\n",
      "287040/287100 [============================>.] - ETA: 0s - loss: 1.1542 - acc: 0.7956\n",
      "Epoch 00020: val_acc did not improve\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "287100/287100 [==============================] - 1762s 6ms/step - loss: 1.1541 - acc: 0.7956 - val_loss: 2.3924 - val_acc: 0.5171\n"
     ]
    }
   ],
   "source": [
    "filepath= \"weights/\" +\"paper-spec\" + \"-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list.append(checkpoint)\n",
    "model_history = final_model.fit(Xtrain, Ytrain, epochs = max_epoch, batch_size = 64,callbacks = callbacks_list ,\n",
    "                                validation_data=(Xval, Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model.save(\"paper-spec-after-20-epoch-weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a547a2290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWLKrGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a41ddb6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.savefig(\"paper-spec.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"paper-spec.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
